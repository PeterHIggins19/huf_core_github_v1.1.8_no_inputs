HUF-DOC: HUF.REL.WIKI.NOTE.WIKI_FOURTH_MONITORING_CATEGORY_EXPANDED | HUF:1.1.8 | DOC:v0.0.0 | STATUS:release | LANE:release | RO:Peter Higgins
CODES: WIKI | ART: CM, AS, TR, EB | EVID:E1 | POSTURE:OP | WEIGHTS: OP=0.80 TOOL=0.20 PEER=0.00 | CAP: OP_MIN=0.51 TOOL_MAX=0.49 | CANON:notes/wiki_pages/wiki_fourth_monitoring_category_expanded.md

# The Fourth Monitoring Category — Expanded
## Self-Referential Non-Invasive Governance State Observation

*Concept expansion — second pass — collective record — February 2026*

---

## What the summary left out

The first pass at this concept established the taxonomy, named the four propositions, and positioned the fourth category against the three established ones. That work is solid. What it does not yet contain is the harder argument: why this category is epistemologically coherent, what it asks of governance actors that is genuinely new, where its limits are, and what the path from concept to confirmed category actually looks like across the next twelve to twenty-four months of research.

This document takes each of those in turn. It does not repeat what the first pass established. It continues from where that pass ended.

---

## Part I — The Epistemological Foundation

### Why self-reference is not circular reasoning

The obvious objection to a self-referential monitoring category is the circularity charge: if the system provides its own reference, the system can define its own compliance. A corrupt institution declares weights that make its current allocation look intentional. The monitoring tool confirms the declaration. Nothing has been audited.

This objection is correct about the corruption case. It misidentifies what the fourth category is doing.

The fourth monitoring category does not audit compliance with declared intent. It makes declared intent audible. These are different operations. An audit asks whether declared intent was met. The fourth category asks whether anything was done that was not declared. The reference is not the declaration itself — it is the gap between the declaration and the observed allocation. A corrupt institution can declare whatever it wants. What it cannot do is prevent the portfolio change log from recording the difference between what it declared and what it did.

The self-referential property is not the institution evaluating itself against its own standard. It is the instrument making the institution's own standard traceable across time. The standard belongs to the institution. The traceability belongs to the instrument. These are not the same thing.

The analogy from acoustics is precise here. The listening position measurement does not evaluate the loudspeaker against a standard the engineer defines. It reads what the system actually produces at the position where it matters. The engineer's intent — the target response — is separate from the measurement. The measurement reveals the gap. The engineer decides what to do about it. The instrument does not make the judgment. It makes the gap impossible to ignore.

### What the fourth category adds to Ostrom's framework that Ostrom's framework lacks

Ostrom's eight design principles are a theory of institutional structure. They describe the conditions under which common pool resource governance systems endure. The monitoring principle (DP4) specifies that monitoring must be conducted by or accountable to the users of the system. It does not specify what the monitoring instrument should be.

This is the gap the fourth monitoring category fills. Ostrom's framework has always known that monitoring is necessary. It has never specified the instrument. The result is that institutions can satisfy DP4 nominally — by having a monitoring system in place — without the monitoring system being capable of detecting the drift it is supposed to catch.

A mandated monitoring system satisfies DP4 nominally. It is conducted by an accountable body. It reports on a regular schedule. It detects breaches of threshold. It does not detect the slow structural drift in portfolio allocation that precedes the breach by years and that is, by the time the breach occurs, essentially irreversible.

The fourth monitoring category fills this gap not by replacing Ostrom's design principles but by specifying the instrument that makes DP4 operationally effective. The monitoring principle has a new implementation. The implementation is the portfolio change log. The portfolio change log is the instrument Ostrom's framework has always needed and never had.

### The asymmetry of evidence

There is an asymmetry in the evidence that the fourth monitoring category generates that is worth making explicit.

A portfolio change log that shows persistent silent drift is evidence of a governance problem. A portfolio change log that shows no persistent silent drift — all allocation change is either declared or corrected within one cycle — is not evidence that governance is healthy. It is evidence that the monitoring instrument is functioning. These are different claims.

This asymmetry matters for deployment. The fourth category is most valuable when it reveals something problematic. When it reveals nothing problematic, the institution cannot conclude that nothing is wrong — only that nothing is wrong that the instrument can see. The instrument's limits must be stated alongside its capabilities. That is the subject of Part IV of this document.

---

## Part II — What It Asks of Governance Actors

### The declaration requirement

The single new thing the fourth monitoring category asks of governance actors is the declaration. Not a new data stream. Not a new reporting format. Not synchronized observation windows. One thing: before the reporting cycle begins, the operator states the intended portfolio allocation as shares summing to 1.0.

This sounds minimal. It is not. The declaration requirement makes implicit priorities explicit and creates a record against which subsequent allocation is compared. Many governance systems have never stated their portfolio priorities as explicit shares. They have stated priorities in qualitative terms — Site A is a priority because of its transboundary significance, Site B is a priority because of its endemic species diversity — without quantifying what those qualitative priorities imply for the distribution of institutional attention across the portfolio.

The declaration forces that quantification. This is an act of governance, not an act of data management. The institution is being asked to commit, in traceable form, to what it believes the correct allocation should be. The monitoring instrument then determines whether that commitment is being honored.

For institutions that have never stated explicit portfolio allocations, the first declaration cycle is the most revealing. Not because the instrument detects drift — there is no previous cycle to compare against — but because the act of declaration surfaces disagreements that existed implicitly. What share of monitoring effort should the smallest site receive? What weight should endemic species diversity carry relative to total site area? These questions have always been present in governance decisions. The declaration requirement makes them answerable.

### The correction response

The fourth monitoring category generates a signal. The governance actor must respond to the signal. The quality of the response determines whether the system converges toward the ground state or whether it drifts further.

Three response types are possible:

**Intentional reweighting with declaration.** The monitoring system reveals drift. The governance actor examines the drift, determines that the allocation has shifted in a direction that reflects a genuine change in priorities, and records that shift as an intentional decision with a stated rationale. The drift is reclassified from silent to intentional. The portfolio change log is updated. The next cycle's declared weights reflect the new priorities. This is correct governance practice. The instrument has done its job. The actor has done theirs.

**Silent drift correction.** The monitoring system reveals drift. The governance actor examines the drift, determines that no decision was made to produce it — it was administrative gravity, resource constraint, political pressure operating below the threshold of formal decision — and makes a correction. The correction is declared. The reasons for the original drift are recorded. The next cycle begins from the corrected allocation. This is the feedback loop closing. This is how the system converges.

**Non-response.** The monitoring system reveals drift. The governance actor does not respond — either because the drift is not recognized as significant, because the institutional capacity to respond is absent, or because the drift serves interests that benefit from it being undetected. The portfolio change log records the non-response as persistent silent drift. Across subsequent cycles, the drift compounds. The action window narrows. Eventually the drift has progressed to a state where correction requires resources, political capital, or time that exceed what is available.

The fourth monitoring category cannot prevent non-response. It cannot force governance actors to act on the signals it generates. What it can do is make non-response visible and attributable across cycles. A governance record that shows persistent silent drift in a specific direction, across multiple reporting periods, despite a functioning monitoring instrument, is a record of a specific kind of governance failure that has a specific kind of documentary trail.

The trail may not compel correction. But it makes the failure impossible to deny and difficult to attribute to ignorance.

### The institutional memory problem

Governance systems lose institutional memory. Key personnel change. Institutional priorities shift. The reasons for historical allocation decisions are not recorded, or the records are not maintained, or the records cannot be retrieved when needed.

The fourth monitoring category, applied consistently across cycles, creates institutional memory as a structural byproduct. The portfolio change log is not a report about the institution. It is the institution's actual governance history, expressed as a time series of ratio states with classification of every change. As the number of cycles grows, the governance history deepens. An institution that has operated under the fourth monitoring category for ten reporting cycles has a ten-cycle record of its allocation states, its declared intentions, its drift patterns, and its correction responses. That record is institutional memory that cannot be lost when a key person leaves because it is embedded in the governance record, not in any individual's head.

This is a property of the instrument that is not captured in the four propositions. It deserves its own status. Call it the institutional memory theorem: **a governance system operating under the fourth monitoring category accumulates institutional memory at the rate of one portfolio state declaration per reporting cycle, permanently and without additional effort.**

The cumulative value of this memory is non-linear. For the first few cycles, the change log is thin and the trajectory is not yet visible. By cycle six or seven — two complete reporting triennial periods under Ramsar's framework — the portfolio trajectory is visible, the characteristic drift patterns are identifiable, the high-Q elements have been through at least one full period of their cycles, and the governance system can be evaluated not just at a point in time but as a system with a direction.

---

## Part III — The Six Failure Modes the Fourth Category is Designed to See

The Handbook (v1.2.0) identifies six failure modes that the existing monitoring categories cannot detect. They deserve full expansion here because they are the practical argument for the fourth category's existence.

### FM-1: Ratio Blindness

The root cause. A governance system managed by absolute metrics treats each element's score, area, or budget allocation as an independent variable. The consequence: an institution can improve every element's absolute score while simultaneously concentrating institutional attention on a smaller subset of high-visibility elements. The absolute improvement is real. The portfolio drift is also real. Existing monitoring instruments see the first. They are structurally unable to see the second.

The fourth monitoring category sees FM-1 in its primary form — the portfolio share table — and in its directional form — the portfolio change log showing shares concentrating or fragmenting over time.

### FM-2: Silent Reweighting

The mechanism by which FM-1 operates in practice. No governance decision is made to concentrate attention. No policy changes. No formal priorities are revised. Administrative gravity — the tendency for resources and monitoring effort to flow toward visible, accessible, politically salient sites — operates below the threshold of formal governance. Each reporting cycle reflects the previous one, slightly modified. The cumulative effect, across three to five cycles, is a substantial and unreported shift in portfolio allocation.

Silent reweighting is invisible to all three established monitoring categories because none of them are designed to detect the absence of a decision. A mandated monitor checks whether a threshold was breached. A question-driven monitor checks whether the model holds. Neither asks: was there a governance decision that produced this allocation state? The fourth monitoring category asks precisely this question and classifies every change as intentional or silent. FM-2 is its primary detection target.

### FM-3: The Snapshot Error

Covered in depth in the Proposition 7.3 discussion. The systematic underestimation of high-Q elements by single-cycle observation. The error is not random. It is directional: high-Q elements are consistently undervalued relative to their long-term contribution because their contribution is phase-dependent and the observation window is shorter than their characteristic cycle.

The snapshot error compounds with FM-2. If high-Q elements are already being undervalued by the snapshot, and silent reweighting is already moving resources toward low-Q high-visibility elements, the two failure modes reinforce each other. The high-Q elements lose share and their phase-dependent contribution makes them look like they are responding poorly to reduced investment — when in fact they are simply in the wrong phase for observation. The governance system reads its own misallocation as confirmation that the misallocation was correct.

### FM-4: The Concentration Trap

A portfolio that has concentrated into a small number of dominant elements has reduced its effective optionality. High-concentration portfolios are efficient in the short term — resources directed at a few high-visibility sites produce visible, reportable outcomes — and fragile in the long term. When conditions change — a climate event, a political transition, a funding disruption — the institution has few fallback elements. The elements that were deprioritized may no longer be functional. The institution cannot redistribute because there is nothing to redistribute to.

FM-4 is the long-cycle consequence of FM-2 allowed to run unchecked. It is detectable by the fourth monitoring category at the PROOF line — the number of elements required to hold a specified fraction of portfolio mass. A PROOF line of 2 (two elements hold 80% of portfolio mass) is a structural warning. A PROOF line that is decreasing over successive cycles is a trajectory warning.

### FM-5: The Fragmentation Spiral

The opposite of FM-4 but equally damaging. A portfolio that has fragmented across many small elements has diffused institutional attention below the threshold of effective management for any individual element. Each element receives some attention. No element receives sufficient attention for its management to be effective. The portfolio appears diverse — many sites, many programs, many initiatives — while each element is managed at a level below what its ecological or governance function requires.

FM-5 is detected by the inverted reading — the reciprocal formula — which amplifies the signal from small-share elements. A portfolio where many elements are clustered near the lower threshold of attention, without any declared decision to reduce them to that level, is exhibiting fragmentation drift. The inverted PROOF line — the number of elements required to hold 80% of the inverted portfolio mass — increases as fragmentation progresses.

### FM-6: The Orphan Element

The governance consequence of either FM-4 or FM-5 carried to a specific endpoint: an element with a share so small that it is effectively invisible to the governance system. The orphan element continues to exist on paper — it has not been removed from the portfolio, it is still listed in the National Report, it still occupies its designated area — but it receives no meaningful governance attention. Its management plan is not updated. Its monitoring visits do not occur. Its RIS form, if submitted at all, reflects conditions from the previous cycle or the one before.

The orphan element is the specific failure mode that the coverage record is designed to prevent. The coverage record documents what received reduced focus, against what criteria, with what declared reasoning. An element that is approaching orphan status — share declining across multiple cycles, no declared decision justifying the decline, management quality indicators deteriorating — is identifiable in the coverage record before it reaches the point of no return.

---

## Part IV — What the Fourth Category Cannot See

An instrument's credibility depends partly on its honesty about its limits. The fourth monitoring category has four structural limits that must be stated alongside its capabilities.

### It cannot see what is not declared

The unity constraint operates on declared outputs. It can only read what governance actors put into the system. If a governance actor systematically under-reports a site's management difficulties, or over-reports its assessment scores, or fails to declare a resource allocation decision, the portfolio change log will reflect the declared fiction rather than the operational reality.

The fourth monitoring category is not a lie detector. It is a coherence detector. It detects incoherence between declared intent and observed allocation. It cannot detect incoherence between declared allocation and actual management practice at the site level. That is the domain of METT, RAPPAM, and SAT. The fourth category supplements these tools; it does not replace them.

### It cannot see inside a reporting cycle

The portfolio change log compares cycles. It cannot detect drift that begins and ends within a single reporting period — a seasonal reallocation that is reversed before the next report, a resource shift that corrects itself through internal processes before the external reporting deadline. If the cycle is long and the drift is fast, significant allocation changes may occur and self-correct without leaving any trace in the portfolio change log.

This is a specific instance of the Proposition 7.3 observation window problem applied to the instrument itself. The fourth monitoring category has a minimum frequency resolution equal to the inverse of its reporting cycle. Changes faster than this frequency are invisible to it.

The practical implication: the instrument is most effective when reporting cycles are short relative to the governance dynamics being tracked. For systems where the reporting cycle is three years (Ramsar National Reports), dynamics that operate on a timescale of months are below the instrument's resolution. This does not invalidate the instrument — it means that supplementary higher-frequency monitoring may be needed for fast-cycle governance questions.

### It cannot attribute causation

The portfolio change log classifies changes as intentional or silent. It does not explain what produced the silent drift. Administrative gravity is one mechanism. Political pressure is another. Resource constraints a third. The instrument cannot determine which mechanism produced the observed drift — it can only record that drift occurred without a corresponding declared decision.

Attribution of cause requires qualitative investigation that the instrument is not designed to perform. The fourth monitoring category identifies the location and magnitude of governance anomalies. It does not explain them. Explanation requires the governance actor to look at the portfolio change log and ask: what happened in this period that might have produced this allocation shift? The instrument starts that conversation. It cannot finish it.

### It cannot correct for Q-factor changes

The Q-factor of a governance element — the ratio of its characteristic contribution period to its observation window — is not fixed. Climate change alters ecological cycles. Political transitions alter institutional cycles. A site whose characteristic period was annual may, under changed conditions, operate on a multi-year cycle. The fourth monitoring category will continue to apply the Q-factor it has calibrated from historical data to an element whose actual Q has changed. The mismatch will produce systematic underestimation or overestimation of that element's contribution until the Q-factor is recalibrated.

This is not a failure of the instrument. It is a property of any monitoring system that uses historical patterns to assess current states. The calibration must be updated as the system's dynamics change. The instrument makes calibration errors visible — they will appear as persistent unexplained residuals in the portfolio change log — but it cannot correct them automatically. That correction requires governance actor judgment.

---

## Part V — The Three-Domain Confirmation Path

Proposition 7.1 (Domain Invariance) currently has two-domain empirical support: a sourdough loaf and a Croatian wetland portfolio. The next research requirement is a third domain.

### Why three domains and not two

Two-domain support demonstrates that the diagnostic works for more than one domain. Three-domain support demonstrates that it works for domains with no structural similarity to each other — that the diagnostic signatures are a property of the unity constraint, not a coincidence of having chosen two similar finite-budget systems.

The two existing systems are both ecological/material: one is a recipe (a design specification for a physical composition) and one is a land portfolio (a geographic allocation of a physical resource). Both are static at the point of declaration — the loaf is declared before baking, the portfolio is declared at the reporting date. Both are measured in physical units (grams, hectares) that convert straightforwardly to proportional shares.

The third domain should be neither ecological nor material. A software retrieval pipeline and a municipal budget line-item allocation are the two leading candidates. Either would introduce a domain that operates at a completely different scale, on a completely different cycle, with completely different institutional governance structures. When the same diagnostic signatures — PROOF line structure, high-leverage tail elements with System Q properties, mean drift gap classification, action window identification — appear in all three, the domain invariance claim has empirical support across domains that share only the unity constraint.

### The software retrieval pipeline as System C

A multi-tenant AI retrieval pipeline is the structurally richest candidate for the third domain. The budget ceiling is total retrieval mass — the sum of all results returned across all namespaces for a given query. Each namespace's share is the fraction of total results it contributes. The declared weights are the configured retrieval priorities. Silent drift is the gradual shift in namespace retrieval shares that occurs as document aging, indexing changes, and embedding model updates alter the effective weight of each namespace without any corresponding configuration change.

This domain has several properties that make it the ideal third domain:

The reporting cycle is short — days or weeks, not years. This means multiple cycles of data can be generated in the time required to complete one Ramsar National Report cycle. The three-domain confirmation can be produced on a compressed timescale.

The drift mechanisms are well-documented. Document aging, embedding model updates, namespace growth differential — all three produce predictable silent drift patterns that can be compared against the HUF diagnostic predictions. If the diagnostic correctly classifies these patterns as silent drift traceable to specific technical events, it validates Proposition 7.2 (Non-Invasive Sufficiency) in a domain where the ground truth is independently knowable.

The Q-factor structure is clear. Large, stable, frequently-updated namespaces are low-Q elements: they contribute broadly and consistently. Small, specialized, slowly-updated namespaces are high-Q elements: they contribute specifically and cyclically, and they appear low-activity in any single-cycle snapshot. The reciprocal inversion should reveal high-Q namespaces as high-leverage elements in the same way it reveals Crna Mlaka in the Croatian portfolio and yeast in the sourdough loaf.

### The confirmation artifact

When the three-domain comparison exists, it should be published as a single exhibit — Case Study D.2 — in the same format as Exhibit A. The three systems should be declared side by side in canonical HUF Declaration format. The structural signatures should be compared in a single diagnostic table. The finding should be stated as a single sentence:

*The HUF Diagnostic Engine applied Tests T1, T4, T5, and T8 to three systems with no domain overlap — artisanal bread, ecological land portfolio, and software retrieval pipeline — and produced coherent, comparable, formally structured results using the same instrument without modification in all three cases. The unity constraint is domain-invariant. This is Exhibit B.*

---

## Part VI — The Naming Question

The fourth monitoring category does not yet have a proper name. The phrase "Self-Referential Non-Invasive Governance State Observation" is a description. It is accurate and it is necessary for positioning in the literature. It is not a name.

Every established monitoring category has a name. Passive monitoring. Mandated monitoring. Question-driven monitoring. The control theory equivalent has a name: state observer. The ecological equivalent has a name: adaptive monitoring. HUF's category needs a name that is distinct from all of these and that captures the category's defining property.

The defining property is not self-reference. That is a consequence. The defining property is not non-invasiveness. That is a method. The defining property is not the unity constraint. That is the instrument.

The defining property is **ratio state observation**: the monitoring of a system's internal distribution across its elements, expressed as proportional shares, tracked across time.

**Ratio state monitoring** is a candidate name. It is short, technical, and accurate. It distinguishes the category from all three established categories: passive monitoring observes events, mandated monitoring observes thresholds, question-driven monitoring observes hypotheses, and ratio state monitoring observes the internal distribution of system mass.

The name is offered as a candidate, not a commitment. The naming question belongs to the collective review process. What matters for the research record is that the question is on the table: the fourth monitoring category needs a proper name, and the name should be chosen before the category enters the literature, not after.

---

## Part VII — The Convergence Path in Practice

Proposition 7.4 (Convergence Under Closed Feedback) describes a mathematical property of the governance system under ideal feedback conditions. The mathematics paper derives the convergence condition formally. What neither document yet contains is what convergence looks like in practice — what a governance actor actually observes as a portfolio approaches the ground state across successive reporting cycles.

### Cycle 1 — Baseline

The first cycle establishes the portfolio share table and the declared weights. There is no portfolio change log because there is no previous cycle. The mean drift gap is computed as the average absolute difference between observed shares and declared weights. This is the baseline drift reading — not a detection of change over time but a reading of how far the current allocation departs from declared intent.

For the Croatian portfolio at baseline, the mean drift gap is 4.8 percentage points. This classifies the portfolio as in the action window: drift is detectable, the gap between declared intent and observed allocation is above the attention threshold, and the cost of correction is lower now than it will be if the drift continues.

### Cycles 2 and 3 — Trajectory Establishment

By the second and third cycles, the portfolio change log contains enough history to reveal a trajectory. Is the mean drift gap increasing (the portfolio is moving away from declared intent), decreasing (the portfolio is moving toward declared intent), or stable (neither converging nor diverging)? Are specific elements showing persistent silent drift, or is the drift randomly distributed across elements? Is the PROOF line changing?

These questions are not answerable from the baseline alone. They require at least two data points. The governance insight from cycles 2 and 3 is trajectory, not state. A portfolio with a mean drift gap of 4.8pp at baseline that reads 3.9pp at cycle 2 is converging. The same portfolio reading 5.7pp at cycle 2 is diverging. The governance response to these two trajectories is different in kind, not just in degree.

### Cycles 4 through 6 — Q-Factor Characterization

By the fourth through sixth cycles, the portfolio change log contains enough history to begin characterizing the Q-factors of individual elements. An element whose share oscillates with a period of approximately two reporting cycles is a medium-Q element. An element whose share has been declining consistently for four cycles without any declared decision is exhibiting persistent silent drift with a four-cycle time constant. An element whose share appears constant across cycles but whose monitoring quality indicators are declining may be in a slow degradation phase that the share metric alone does not reveal.

This is the point at which the fourth monitoring category begins to function as a true governance state observer rather than a drift detector. The governance actor is no longer just asking whether the allocation has changed. They are asking what pattern of change characterizes each element, what that pattern implies about the element's governance trajectory, and whether the pattern is consistent with the declared intent for that element.

### Cycles 7 and beyond — Ground State Approach

A portfolio that has been under fourth monitoring category observation for seven or more cycles — approximately twenty years of Ramsar triennial reporting — has a governance record that spans multiple institutional generations. The record shows which allocation decisions were deliberate and which were silent. It shows which elements were at risk of orphan status and whether intervention occurred. It shows whether the mean drift gap has been declining over time (convergence) or holding steady (equilibrium) or increasing (structural drift).

A portfolio at the ground state — mean drift gap near zero, PROOF line stable or improving, no persistent silent drift in any element, correction responses occurring within one cycle of detection — has demonstrated self-governance in the practical sense. It does not need an external audit to tell it whether it is functioning as intended. The portfolio change log tells it. The accountability trail is complete. The institutional memory is intact. The action window is open by default because the monitoring instrument is continuously tracking whether any window is closing.

This is what Proposition 7.4 describes in mathematical terms. This is what it looks like in governance practice. The convergence condition is not a sudden transition. It is a gradual approach. It is visible in the data before it is reached. The data tells the governance actor when they are getting close.

---

## What the concept now contains that it did not before

The first pass established the taxonomy, the four propositions, and the comparative positioning. This pass adds:

**The epistemological argument** — self-reference is not circular reasoning because the instrument makes the declaration traceable, not the declaration self-validating.

**What Ostrom's framework was missing** — the fourth category fills the instrument gap that DP4 has always had.

**The asymmetry of evidence** — absence of detected drift is not proof of health; it is proof of instrument function.

**The declaration as governance act** — not data management, but commitment made traceable.

**The six failure modes in full** — FM-1 through FM-6, each distinct, each invisible to the three established categories.

**The four structural limits** — what it cannot see, stated clearly.

**The three-domain confirmation path** — why software retrieval is the right third domain and what the confirmation artifact should look like.

**The naming question** — ratio state monitoring as a candidate name, offered to the collective.

**Convergence in practice** — what seven cycles of observation looks like from inside the governance system.

The concept is now substantially complete. The formalization and empirical work are the remaining steps.

---

*Nothing claims more than the artifacts support.*
*Peter Higgins · Rogue Wave Audio · Markham, Ontario · roguewaveaudio.com*
*HUF v1.2.0 · MIT License · February 2026*
