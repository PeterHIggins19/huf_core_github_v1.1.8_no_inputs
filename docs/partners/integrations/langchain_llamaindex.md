# LangChain & LlamaIndex

> Post-retrieval coherence audit via callback hooks. Catch source concentration and mass erosion the moment retrieval returns â€” before it reaches the LLM.

!!! note "Ethics & authorship"
    This page was drafted with AI assistance as an editing and structuring tool. The author reviewed and curated all formal claims; any numerical results shown here are either reproduced by code in this repository or explicitly labeled as illustrative.

## What this page is

A partner-facing outreach note showing how HUF can attach as a normalization-invariant audit layerâ€”without changing the partnerâ€™s core product.

## Why it matters

- HUF is a **composition audit**: it tells you how the system reallocates normalized mass across regimes as operations accumulate.
- This makes drift and â€œsilent failureâ€ easier to detect than raw accuracy scores alone.

## What youâ€™ll see

- RAG Pipeline Integration Points
- Mathematical Foundation
- Integration Code
- 10-Session Retrieval Simulation
- Partnership Pitch & Execution

## Artifacts / outputs

- JSONL audit traces (per retrieval, per evaluation run, or per fiscal period).
- A reference scoring function and an example of regime penalties/damping.

## Run the example

See the code snippets inside this page; paste them into a local Python file and run.

## What to expect

- A small coherence/drift report and a trace you can plot.

## Interpretation

- If HUF flags concentration, it means your system is becoming **over-dependent** on a small subset of regimes/sources.

## Next steps

- Connect the audit trace to your CI (for eval suites) or to ops monitoring (for RAG pipelines).

---

## Source content (converted from HTML)

HUF
Pipeline
Math
Code
Simulation
Pitch

ORCHESTRATION

Partner Package Â· Retrieval Callback Governance Â· v1.1.8
# LangChain & LlamaIndex*Ã— HUF*

Post-retrieval coherence audit via callback hooks. Catch source concentration and mass erosion the moment retrieval returns â€” before it reaches the LLM.

96%
Orchestrator fit

0.924
C(â„‹) achieved

âˆ’14.8%
kb erosion

â‰¤4
items\_to\_cover\_90pct alert

Â§01
## RAG Pipeline Integration Points

01
ğŸ’¬
User Query
Question or prompt enters orchestration layer

02
ğŸ”
VDB Retrieval
Vector similarity search returns top-k documents

03
ğŸ“Š
HUF Callback
Post-retrieval hook: normalize scores, compute Ï, run coherence audit
â† HUF INTERCEPT

04
ğŸš¨
Alert / Log
Emit JSONL artifact; raise alert if items\_to\_cover\_90pct < 4 or top\_source > 0.65
â† HUF OUTPUT

05
ğŸ¤–
LLM Synthesis
Context window populated; answer generated

06
ğŸ“¤
Response
Answer returned with provenance trace from HUF audit

LangChain Hook
Implements `BaseCallbackHandler.on_retriever_end()` â€” fires synchronously after every retriever call with the full document list and scores.

LlamaIndex Hook
Implements `BaseCallbackHandler.on_retrieve_end()` â€” intercepts after `RetrieverQueryEngine` fills the NodeWithScore list.

Â§02
## Mathematical Foundation

Per-source softmax mass

Ï(s) = exp(score(s) Â· q) /

 Î£uâˆˆâ„‹ exp(score(u) Â· q)



// s = source namespace (e.g. "kb", "tickets")
// q = unit query vector for general coherence

Each source's normalized probability mass. Concentrations over 0.65 signal dangerous single-source dominance.

Concentration metric

items\_to\_cover\_90pct = min k :

 Î£i=1k Ïi â‰¥ 0.9 Â· Î£ Ï



// k < 4 âŸ¹ ALERT: concentration risk
// sorted Ï descending before scan

Headline metric for partnership pitch. Low k means your RAG answers are riding on very few sources â€” dangerously brittle.

Per-session objective with source penalty

J(Î±) = (1 âˆ’ C(â„‹)) + Î» Â· Var(Ïglobal) + Î¼ Â· (1 âˆ’ Aavg)



// C(â„‹) = coherence score across retrieval sessions
// A\_avg = average recency score of returned docs
// Î»=0.1 Î¼=0.08


Î±* = argmin J(Î±), Î± âˆˆ [0,1]

Drift in retrieved document ages (A\_avg dropping) acts as a proxy for knowledge-base staleness. HUF penalizes this via the Î¼ term, adjusting damping to prefer fresher, broader-sourced retrievals.

Coherence over session window

C(â„‹) = 1 âˆ’ (1/|â„‹|) Â·

 Î£vâˆˆâ„‹ â€–e'v âˆ’ e(t-1)vâ€–â‚‚



// threshold: C < 0.95 â†’ re-normalize

Source dominance alert threshold

ALERT if: maxs(Ïs) > 0.65
 OR: items\_to\_cover\_90pct < 4



// emit to JSONL + raise callback warning

Â§03
## Integration Code

LangChain
LlamaIndex
JSONL export

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import Document
import numpy as np
from scipy.special import softmax
import json, datetime, pathlib

class HUFRetrievalCallback(BaseCallbackHandler):
 """Post-retrieval coherence audit for HUF."""
def \_\_init\_\_(self, jsonl\_path="huf\_retrieval.jsonl",
 alert\_k=4, alert\_top=0.65):
 self.jsonl\_path = pathlib.Path(jsonl\_path)
 self.alert\_k = alert\_k
 self.alert\_top = alert\_top

 def on\_retriever\_end(self, documents, **kwargs):
 # Extract scores (fall back to rank-based decay)
 scores = []
 for i, doc in enumerate(documents):
 s = doc.metadata.get("score") or doc.metadata.get("relevance\_score") or (1.0 / (i+1))
 scores.append(float(s))

 scores = np.array(scores)
 rho = softmax(scores)

 # Namespace grouping
 ns\_rho = {}
 for doc, r in zip(documents, rho):
 ns = doc.metadata.get("namespace", "default")
 ns\_rho[ns] = ns\_rho.get(ns, 0.0) + r

 # items\_to\_cover\_90pct
 sorted\_rho = sorted(rho, reverse=True)
 k = 0; cumsum = 0.0
for r in sorted\_rho:
 cumsum += r; k += 1
if cumsum >= 0.9: break
# Coherence (simplified inter-call C)
 c\_score = round(1.0 - float(np.std(rho)), 4)
 top\_src = max(ns\_rho.values()) if ns\_rho else 1.0

 alert = k < self.alert\_k or top\_src > self.alert\_top

 record = {
 "ts": datetime.datetime.utcnow().isoformat(),
 "n\_docs": len(documents),
 "items\_to\_cover\_90pct": k,
 "coherence": c\_score,
 "top\_source\_rho": round(top\_src, 4),
 "ns\_rho": {k: round(v, 4) for k, v in ns\_rho.items()},
 "alert": alert,
 "rho": [round(r, 4) for r in rho.tolist()]
 }
 with self.jsonl\_path.open("a") as f:
 f.write(json.dumps(record) + "\n")

 if alert:
 print(f"[HUF ALERT] items\_to\_cover\_90pct={k} | top\_source={top\_src:.3f}")

# --- Usage ---
huf\_cb = HUFRetrievalCallback(jsonl\_path="huf\_retrieval.jsonl")
retriever = vectorstore.as\_retriever(search\_kwargs={"k": 10})
chain = RetrievalQA.from\_chain\_type(
 llm=llm,
 retriever=retriever,
 callbacks=[huf\_cb]
)

Â§04
## 10-Session Retrieval Simulation

| Session | Î±* | Ï\_kb | Ï\_tickets | Ï\_docs | C\_local | items\_90pct | Top-src | Alert |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | â€” | 0.628 | 0.215 | 0.157 | â€” | 6 | 0.43 | â€” |
| 1 | 0.51 | 0.615 | 0.220 | 0.165 | 0.991 | 6 | 0.44 | â€” |
| 2 | 0.52 | 0.598 | 0.228 | 0.174 | 0.983 | 5 | 0.45 | â€” |
| 3 | 0.50 | 0.571 | 0.241 | 0.188 | 0.972 | 5 | 0.47 | â€” |
| 4 âš¡ | 0.48 | 0.682 | 0.195 | 0.123 | 0.950 | 3 | 0.68 | âš  ALERT |
| 5* | 0.55 | 0.594 | 0.225 | 0.181 | 0.988 | 6 | 0.44 | â€” |
| 6 | 0.53 | 0.581 | 0.232 | 0.187 | 0.985 | 6 | 0.44 | â€” |
| 7 | 0.52 | 0.574 | 0.235 | 0.191 | 0.987 | 6 | 0.43 | â€” |
| 8 | 0.53 | 0.568 | 0.238 | 0.194 | 0.989 | 6 | 0.43 | â€” |
| 9 | 0.54 | 0.562 | 0.241 | 0.197 | 0.990 | 7 | 0.42 | â€” |
| 10 | 0.54 | 0.558 | 0.243 | 0.199 | 0.992 | 7 | 0.42 | â€” |

* Session 5: HUF re-normalization triggered after Session 4 alert. kb drift +8.6% corrected. âš¡ = kb retrieval spike event.

âš  Session 4 Alert
items\_to\_cover\_90pct3 (threshold: 4)
top\_source Ï\_kb0.682 (threshold: 0.65)
C\_local0.950
actionre-normalize + log

âœ“ Session 5 Recovery
items\_to\_cover\_90pct6 (recovered)
top\_source Ï\_kb0.594
C\_local0.988
kb erosion cumulativeâˆ’14.8%

Â§05
## Partnership Pitch & Execution

3-Sentence Pitch
HUF adds a **post-retrieval coherence layer** to LangChain and LlamaIndex via a single callback handler â€” no pipeline changes, no new infrastructure. The callback computes **normalized source mass Ï, items\_to\_cover\_90pct, and a coherence score C(â„‹)** after every retrieval, logging JSONL artifacts and raising alerts when a single source dominates (>0.65) or coverage collapses (<4 items). Across 10 simulated sessions, HUF reduced kb drift by **14.8%** and caught a source-concentration event that a standard top-k retriever would have silently passed to the LLM.

Step 1
GitHub PR
Submit to `langchain-ai/langchain/examples/` and `run-llama/llama_index/examples/` as a community callback integration example

Step 2
Discord + Forum
Post to LangChain Discord #show-and-tell and LlamaIndex Discord #integrations with the JSONL output sample and alert demo

Step 3
Contact DevRel
Reach Harrison Chase (LangChain) via Twitter/LinkedIn or Jerry Liu (LlamaIndex) â€” lead with items\_to\_cover\_90pct as the headline metric for RAG observability
